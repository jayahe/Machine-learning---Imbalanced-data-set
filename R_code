# High dimensional imbalanced data set 
# Date : 26/12/2016

## Step1 - loading data

path <- "D:/project1"
setwd(path)

library(data.table)
train <- fread("train.csv",na.strings = c(""," ","?",NA))
test <- fread("test.csv", na.strings = c(""," ","?",NA))

# Look at the data
dim(train) # 199523 / 41
dim(test) # 99762 / 41
str(train)
str(test)

head(train)
head(test)

# check target/ dependant varible

unique(train$income_level) # "-50000" "+50000"
unique(test$income_level) # "-50000" "+50000"

# Encode target variables

train[,income_level := ifelse(income_level == "-50000",0,1)]
test[,income_level := ifelse(income_level == "-50000",0,1)]

round(prop.table(table(train$income_level))*100) # 94% majority class ; 6% minority class
round(prop.table(table(test$income_level))*100)

# Set the column classes

factcols <- c(2:5,7,8:16,20:29,31:38,40,41)
numcols<- setdiff(1:40,factcols)

train[,(factcols):= lapply(.SD,factor),.SDcols =factcols]
train[,(numcols):= lapply(.SD, as.numeric), .SDcols = numcols]

test[,(factcols):= lapply(.SD,factor),.SDcols =factcols]
test[,(numcols):= lapply(.SD, as.numeric), .SDcols = numcols]

# lets separate categorical and numerical variables
#subset categorical variables
cat_train <- train[,factcols,with=F]
cat_test <- test[,factcols, with=F]

#subset numerical variables
num_train<- train[,numcols, with=F]
num_test <- test[,numcols, with =F]

rm(train) # removing train and test data will be helpful to use our memory for some other purpose
rm(test)

## Step2-  Data Exploration - Numerical data

library(ggplot2)
install.packages("plotly")
library(plotly)

# Write a plot function

tr <- function(a){ ggplot(data = num_train, aes(x = a, y =..density..))
+geom_histogram(fill = "blue", color = 'red', alpha = 0.5, bins = 100)
+ geom_density() } # Add  ggplotly() to make it more interactive

tr(num_train$age)

tr(num_train$capital_losses)

# Compare numeric variables along with dependant variables - adding dependant variable to the train data

num_train[,income_level := cat_train$income_level]

# create scatterplot

ggplot(data = num_train, aes(x = age, y = wage_per_hour))
+ geom_point(aes(color = income_level))
+ scale_y_continuous("wage per hour", breaks = seq(0,10000,1000))

# plot categorical variable against dependant variable

# dodged bar chart instead of bar plot

all_bar <- function(i){ ggplot(data= cat_train, aes(x = i, fill = income_level ))
+ geom_bar(position = 'dodge', color = 'black')+ scale_fill_brewer(palette = 'Pastel1')
+theme(axis.text.x = element_text(angle = 60,hjust = 1,size = 10))}

# categorical vs continuous
all_bar(cat_train$class_of_worker) 

all_bar(cat_train$education)

# categorical vs cateorical
prop.table(table(cat_train$marital_status, cat_train$income_level),1)

prop.table(table(cat_train$class_of_worker, cat_train$income_level),1)

## Step3 - Data cleaning

# chek missing values in numeric variables

table(is.na(num_train)) # the numeric variables have no missing values
table(is.na(num_test))

# Check for correlated variables

library(caret)
# set correlation threshold as 0.7

num_train$income_level <- as.numeric(num_train$income_level)

ax <- findCorrelation(x = cor(num_train),cutoff = 0.7)
ax

num_train <- num_train[,-ax,with = F]
num_test <- num_test[,weeks_worked_in_year := NULL]

# Check missing values for categorical varibales

table(is.na(cat_test))
mvtr<- sapply(cat_train, function(x){sum(is.na(x)/length(x))}*100)
mvte <- sapply(cat_test, function(x){sum(is.na(x)/length(x))}*100)
mvtr
mvte

cat_train <- subset(cat_train, select = mvtr< 5)
cat_test <- subset(cat_test, select = mvte < 5)


# set NA as 'Unavailable' - train data

#convert to characters
cat_train <- cat_train[,names(cat_train) := lapply(.SD, as.character),.SDcols = names(cat_train)]
for(i in seq_along(cat_train)) set(cat_train, i=which(is.na(cat_train[[1]])),j=i, value = 'Unavailable')

# convert back to factors
cat_train <- cat_train[,names(cat_train):= lapply(.SD,factor),.SDcols= names(cat_train)]


# set NA as 'Unavailable' - test data

#convert to characters
cat_test <- cat_test[,names(cat_test) := lapply(.SD, as.character),.SDcols = names(cat_test)] 

for(i in seq_along(cat_test)) set(cat_test, i=which(is.na(cat_test[[1]])),j=i, value = 'Unavailable')

# convert back to factors
cat_test <- cat_test[,names(cat_test):= lapply(.SD,factor),.SDcols= names(cat_test)]

## Step4 - Data Manipulation
# combine factor levels with less than 5% values
# train data
for(i in names(cat_train)) { 
      p<- 5/100
      Id <- names(which(prop.table(table(cat_train[[1]])) < p))
      levels(cat_train[[i]])[levels(cat_train[[i]] %in% Id)] <- 'Other'}

# test data
for(i in names(cat_test)) { 
  p<- 5/100
  Id <- names(which(prop.table(table(cat_test[[1]])) < p))
  levels(cat_test[[i]])[levels(cat_test[[i]] %in% Id)] <- 'Other'}


# check columns with unequal levels
install.packages('mlr')
install.packages('httpuv')
library(httpuv)
library(mlr)

summarizeColumns(cat_train)[,'nlevs']
summarizeColumns(cat_test)[,'nlevs']

#  Numeric variables
num_train[,.N,age][order(age)]
num_train[,.N,wage_per_hour][order(-N)] # (N) - ascending order ; (-N) - descending order

# bin age variable 0-30 31-60 61-90
num_train[,age:= cut(x = age, breaks = c(0,30,60,90),include.lowest = T,labels = c('young','adult','old'))]
num_train[,age := factor(age)]

num_test[,age := cut(x = age, breaks = c(0,30,60,90), include.lowest = T, labels = c('young','adult','old'))]
num_test[,age := factor(age)]

# Bin numeric variables with 0 and morethan 0
num_train [, wage_per_hour := ifelse(wage_per_hour == 0,'Zero','MorethanZero')][,wage_per_hour := as.factor(wage_per_hour)]

num_train[,capital_gains := ifelse(capital_gains == 0, 'Zero','morethanZero')][,capital_gains := as.factor(capital_gains)]

num_train[,capital_losses := ifelse(capital_losses == 0, 'Zero','morethanZero')][,capital_losses:= as.factor(capital_losses)]

num_train[,dividend_from_Stocks := ifelse(dividend_from_Stocks == 0, 'Zero','morethanZero')][,dividend_from_Stocks := as.factor(dividend_from_Stocks)]

#test
num_test [, wage_per_hour := ifelse(wage_per_hour == 0,'Zero','MorethanZero')][,wage_per_hour := as.factor(wage_per_hour)]

num_test[,capital_gains := ifelse(capital_gains == 0, 'Zero','morethanZero')][,capital_gains := as.factor(capital_gains)]

num_test[,capital_losses := ifelse(capital_losses == 0, 'Zero','morethanZero')][,capital_losses:= as.factor(capital_losses)]

num_test[,dividend_from_Stocks := ifelse(dividend_from_Stocks == 0, 'Zero','morethanZero')][,dividend_from_Stocks := as.factor(dividend_from_Stocks)]

# Remove the dependant variable from num_train data
num_train[,income_level := NULL]

## Step5 - Machine learning - Model building

# Combine data and make train and test files
d_train <- cbind(num_train, cat_train)
d_test <- cbind(num_test, cat_test)

# remove unwanted data to save memory
rm(num_test,num_train,cat_test,cat_train)

library(mlr)

# creare task
train_task <- makeClassifTask(data = d_train,target = 'income_level')
test_task <- makeClassifTask(data = d_test, target = 'income_level')
makeclass
# remove zero variance features - constants
train_task <- removeConstantFeatures(train_task)
test_task <- removeConstantFeatures(test_task)

# get variable importance chart

var_imp <- generateFilterValuesData(train_task, method = c('information.gain')) # encountered memory issues

# Make the data balanced - majority and minority classes
#a) undersampling
train.under <- undersample(train_task, rate = 0.1) # keep only 10% majority class
table(getTaskTargets(train.under))

#b) oversampling
train.over<- oversample(train_task, rate = 15) # make minority class 15 times
table(getTaskTargets(train.over))

#c) SMOTE - synthetic data generation

train.smote <- smote(train_task,rate = 15, nn=5) # may take more computatinal time
# lets modify the parameters and run it again

system.time(train.smote <- smote(train_task,rate = 10, nn=3))

# lets see which algorithms are available to solve the problem
listLearners('classif','twoclass')[c('class','package')] # 34 algorithms listed

## Using naive bayes on all 4 datasets and compare the prediciton accuracy using cross validation
naive.learner <- makeLearner('classif.naiveBayes',predict.type = 'response')
naive.learner$par.vals <- list(laplace = 1)

# 10fold cv - stratified
folds <- makeResampleDesc('CV',iters=10,stratify = T)

# cross validation funciton
fun.cv <- function(a){ crv_val <- resample(naive.learner,a,folds,measures = list(acc,tpr,fpr,tnr,fp,fn))
crv_val$aggr}

fun.cv(train_task)

fun.cv(train.under)

fun.cv(train.over)

fun.cv(train.smote) # smote data gives better accuracy compared to others

# train and predict data
nb.model <- train(naive.learner,train.under)
nb.predict <- predict(nb.model,test_task)

nb.prediction <- nb.predict$data$response
dcm <- confusionMatrix(d_test$income_level,nb.prediction)

# Calculate F measure
precision <- dcm$byClass['Pos Pred Value']
recall <- dcm$byClass['Sensitivity']

f.measure <- 2*((precision * recall)/(precision + recall))
f.measure

## Xgboost

set.seed(1000)
xgb.learner <- makeLearner('classif.xgboost',predict.type = 'response')
xgb.learner$par.vals <- list(
                             objective = 'binary:logistic', eval_metric = 'error',nrounds = 150,print.every.n = 50)

# define hyperparameters for tuning 

xg_ps <- makeParamSet(
                    makeIntegerParam('max_depth',lower = 3,upper = 10),
                    makeNumericParam('lambda',lower = 0.05,upper = 0.5),
                    makeNumericParam('eta',lower = 0.01, upper = 0.5),
                    makeNumericParam('subsample',lower = 0.5,upper = 1),
                    makeNumericParam('min_child_weight',lower = 2,upper = 10),
                    makeNumericParam('colsample_bytree',lower = 0.5,upper = 0.8)
)

# define search function
rancontrol <- makeTuneControlRandom(maxit = 5L) # do 5 iterations
# 5 fold cross validation
set.cv <- makeResampleDesc('CV',iters = 5L, stratify = T)

# tune parameters

xgb_tune <- tuneParams(learner = xgb.learner, task = train_task,resampling = set.cv,measures = list(acc,tpr,tnr,fpr,fp,fn),par.set = xg_ps,control = rancontrol)

# set optimal parameters

xgb_new <- setHyperPars(learner = xgb.learner, par.vals = xgb_tune$x)

# train model
xgbmodel <- train(xgb_new,train_task)

# test model

predict.xg <- predict(xgbmodel, test_task)

# make predictions
xg_prediction <- predict.xg$data$response

# make confusion matrix
xg_confused <- confusionMatrix(d_test$income_level, xg_prediction)

precision <- xg_confused$byClass['Pos Pred Value']
recall <- xg_confused$byClass['Sensitivity']

f.measure <- 2*((precision * recall)/(precision + recall))
f.measure

# top 20 features

filtered.data <- filterFeatures(train_task,method = 'information.gain',abs = 20)

# train
xgb_boost <- train(xgb_new,filtered.data)
 
predict.xg$threshold # threshold of 0.5 always favor the majority class

# Xgboost AUC

xgb_prob <- setPredictType(learner = xgb_new,predict.type = 'prob')

# train model
xgmodel_prob <- train(xgb_prob, train_task)

# predict

predict.xgprob <- predict(xgmodel,test_task)

# predicted probabilities
predict.xgprob$data[1:10,]

df <- generateThreshVsPerfData(predict.xgprob, measures = list(fpr,tpr))
plotROCCurves(df)

# set threshold as 0.4

pred2 <- setThreshold(predict.xgprob,0.4)

confusionMatrix(d_test$income_level,pred2$data$response)

pred3 <- setThreshold(predict.xgprob,0.3)

confusionMatrix(d_test$income_level, pred3$data$response)

# xgb model is better as it predicts 77% minority classes correctly
# try building model with svm
